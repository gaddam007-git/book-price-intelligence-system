{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEJuS1TMiIWORAGX/z8Y7q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaddam007-git/book-price-intelligence-system/blob/main/Notebooks/milestone_2_Webscrapping_and_data_aggregation/milestone_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "nJ_7NMkfKocU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Milestone 2: Webscrapping and data aggregation.**"
      ],
      "metadata": {
        "id": "p2WODIosHQo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch --quiet"
      ],
      "metadata": {
        "id": "31h86YM0HNiM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pipeline utility from the Hugging Face transformers library\n",
        "# pipeline provides a simple high-level API for common NLP tasks\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained Question Answering (QA) model\n",
        "# \"question-answering\" specifies the task type\n",
        "# \"distilbert-base-cased-distilled-squad\" is a lightweight DistilBERT model\n",
        "# fine-tuned on the SQuAD dataset for extracting answers from text\n",
        "qa_model = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"distilbert-base-cased-distilled-squad\"\n",
        ")\n",
        "\n",
        "# Context paragraph on which the model will search for the answer\n",
        "# This acts as the knowledge base for the QA system\n",
        "context = \"\"\"\n",
        "Machine learning is a field of artificial intelligence that uses statistical techniques\n",
        "to give computer systems the ability to learn from data. It focuses on developing\n",
        "algorithms that improve automatically through experience. Popular ML methods include\n",
        "supervised learning, unsupervised learning, and reinforcement learning.\n",
        "\"\"\"\n",
        "\n",
        "# Define the question that will be answered using the context above\n",
        "# The model will look for the most relevant text span as the answer\n",
        "question = \"What are popular machine learning methods?\"\n",
        "\n",
        "# Run the QA model by passing the question and context\n",
        "# The model returns:\n",
        "# - 'answer': extracted text span\n",
        "# - 'score': confidence score for the predicted answer\n",
        "# - 'start' and 'end': character positions of the answer in the context\n",
        "result = qa_model(question=question, context=context)\n",
        "\n",
        "# Print the question to the console\n",
        "print(\"Question:\", question)\n",
        "\n",
        "# Print the extracted answer from the context\n",
        "print(\"Answer:\", result['answer'])\n",
        "\n",
        "# Print the confidence score indicating how sure the model is\n",
        "print(\"Score:\", result['score'])"
      ],
      "metadata": {
        "id": "e0RoapJ8HUO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cddc3911-c50c-448b-9bda-13c0606a6fe3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are popular machine learning methods?\n",
            "Answer: supervised learning, unsupervised learning, and reinforcement learning\n",
            "Score: 0.7955703735351562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pipeline function from Hugging Face's transformers library\n",
        "# pipeline provides a simple interface to use pre-trained NLP models\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained Question Answering (QA) transformer model\n",
        "# \"question-answering\" specifies the task type\n",
        "# \"distilbert-base-cased-distilled-squad\" is a DistilBERT model\n",
        "# fine-tuned on the SQuAD dataset for extracting answers from text\n",
        "qa_model = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"distilbert-base-cased-distilled-squad\"\n",
        ")\n",
        "\n",
        "# Context paragraph that acts as the knowledge base\n",
        "# The model will only search for answers within this text\n",
        "context = \"\"\"\n",
        "The Internet has become an essential part of modern life, connecting millions of devices\n",
        "around the world. It allows people to communicate instantly, access information, and\n",
        "perform online transactions. Over the years, internet technology has evolved from simple\n",
        "web pages to advanced cloud computing services. With the rise of smartphones, people\n",
        "now use the internet daily for navigation, social media, entertainment, and remote work.\n",
        "Despite its benefits, internet users must be cautious about cybersecurity threats such\n",
        "as phishing, malware, and data breaches.\n",
        "\"\"\"\n",
        "\n",
        "# ---- QUESTIONS ----\n",
        "\n",
        "# Question whose answer is explicitly present in the context\n",
        "q1 = \"What are some common cybersecurity threats mentioned in the context?\"\n",
        "\n",
        "# Question whose answer is NOT present in the context\n",
        "# The model will still try to guess an answer based on closest text\n",
        "q2 = \"What is the capital of India?\"\n",
        "\n",
        "# Another out-of-context question\n",
        "# Demonstrates how QA models behave when the answer is missing\n",
        "q3 = \"What is the Unit of Internet?\"\n",
        "\n",
        "# Store all questions in a list for batch processing\n",
        "questions = [q1, q2, q3]\n",
        "\n",
        "# ---- RUN ALL QUESTIONS ----\n",
        "\n",
        "# Loop through each question with an index starting from 1\n",
        "for i, q in enumerate(questions, 1):\n",
        "\n",
        "    # Pass the question and context to the QA model\n",
        "    # The model returns a dictionary containing:\n",
        "    # - 'answer': extracted text span\n",
        "    # - 'score': confidence score of the prediction\n",
        "    # - 'start' and 'end': character positions in the context\n",
        "    result = qa_model(question=q, context=context)\n",
        "\n",
        "    # Print the question number and text\n",
        "    print(f\"\\nQuestion {i}: {q}\")\n",
        "\n",
        "    # Print the model's extracted answer\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "\n",
        "    # Print the confidence score for the predicted answer\n",
        "    print(\"Score:\", result[\"score\"])"
      ],
      "metadata": {
        "id": "ZGYQd9ysHX1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec21a6f5-6d4f-4276-cb20-635227cbd83c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1: What are some common cybersecurity threats mentioned in the context?\n",
            "Answer: phishing, malware, and data breaches\n",
            "Score: 0.922915115748765\n",
            "\n",
            "Question 2: What is the capital of India?\n",
            "Answer: connecting millions of devices\n",
            "around the world\n",
            "Score: 0.0007901894859969616\n",
            "\n",
            "Question 3: What is the Unit of Internet?\n",
            "Answer: allows people to communicate instantly, access information, and\n",
            "perform online transactions\n",
            "Score: 0.20774200558662415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATION FOR THE ABOVE CODE\n",
        "\n",
        "1. Explicit questions work correctly.\n",
        "  * The first question (“What are some common cybersecurity threats…”) is directly answerable from the given context.\n",
        "  * The model gives the correct answer with a high confidence score (0.92).\n",
        "\n",
        "2. Implicit or unrelated questions fail.\n",
        "  * The second question (“What is the capital of India?”) is not present in the context, so the model tries to guess something anyway.\n",
        "  * It returns a meaningless answer with a very low score (0.00079), showing that it is unreliable for out-of-context questions.\n",
        "\n",
        "3. Vague or unclear questions also give incorrect results.\n",
        "  * The third question (“What is the Unit of Internet?”) has no clear meaning and is not in the context.\n",
        "  * The model extracts random text with a low-to-medium score (0.20), indicating uncertainty.\n",
        "\n",
        "CONCLUSION\n",
        "\n",
        "The QA model performs well only when the question is explicitly answerable from the given context . If the question is unrelated, implicit, or unclear, the model still tries to answer but produces incorrect and low-confidence outputs.\n"
      ],
      "metadata": {
        "id": "Vbc76EiGHcd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pipeline utility from the Hugging Face transformers library\n",
        "# pipeline provides an easy-to-use interface for running pre-trained NLP models\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained Question Answering (QA) model\n",
        "# \"question-answering\" specifies the NLP task\n",
        "# \"distilbert-base-cased-distilled-squad\" is a lightweight DistilBERT model\n",
        "# trained on the SQuAD dataset for extractive question answering\n",
        "qa_model = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"distilbert-base-cased-distilled-squad\"\n",
        ")\n",
        "\n",
        "# Context paragraph that acts as the knowledge source\n",
        "# The model will extract answers only from this text\n",
        "context = \"\"\"\n",
        "The Internet has become an essential part of modern life, connecting millions of devices\n",
        "around the world. It allows people to communicate instantly, access information, and\n",
        "perform online transactions. Over the years, internet technology has evolved from simple\n",
        "web pages to advanced cloud computing services. With the rise of smartphones, people\n",
        "now use the internet daily for navigation, social media, entertainment, and remote work.\n",
        "Despite its benefits, internet users must be cautious about cybersecurity threats such\n",
        "as phishing, malware, and data breaches.\n",
        "\"\"\"\n",
        "\n",
        "# ---- QUESTIONS ----\n",
        "\n",
        "# Explicit question: the answer clearly exists in the context\n",
        "q1 = \"What are some common cybersecurity threats mentioned in the context?\"\n",
        "\n",
        "# Implicit / out-of-context question\n",
        "# The answer is not present in the context\n",
        "q2 = \"What is the capital of India?\"\n",
        "\n",
        "# Another out-of-context question\n",
        "q3 = \"What is the Unit of Internet?\"\n",
        "\n",
        "# Store all questions in a list for iteration\n",
        "questions = [q1, q2, q3]\n",
        "\n",
        "# ---- RUN ALL QUESTIONS ----\n",
        "\n",
        "# Loop through each question with an index starting from 1\n",
        "for i, q in enumerate(questions, 1):\n",
        "\n",
        "    # Pass the question and context to the QA model\n",
        "    # The model returns a dictionary containing:\n",
        "    # - 'answer': extracted text span\n",
        "    # - 'score': confidence score of the prediction\n",
        "    result = qa_model(question=q, context=context)\n",
        "\n",
        "    # Extract the confidence score from the result\n",
        "    score = result[\"score\"]\n",
        "\n",
        "    # Extract the predicted answer from the result\n",
        "    answer = result[\"answer\"]\n",
        "\n",
        "    # Print the question number and text\n",
        "    print(f\"\\nQuestion {i}: {q}\")\n",
        "\n",
        "    # Print the confidence score\n",
        "    print(\"Score:\", score)\n",
        "\n",
        "    # ---- IF-ELSE CHECK ----\n",
        "\n",
        "    # If confidence score is high (>= 0.8),\n",
        "    # treat the answer as reliable and display it\n",
        "    if score >= 0.8:\n",
        "        print(\"Answer:\", answer)\n",
        "\n",
        "    # If confidence score is low,\n",
        "    # indicate that the answer is not relevant\n",
        "    else:\n",
        "        print(\"Answer is not relevant to the question asked.\")"
      ],
      "metadata": {
        "id": "BmDwjxw3HjMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9a2c38-134b-49a4-e125-de18310ade8b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1: What are some common cybersecurity threats mentioned in the context?\n",
            "Score: 0.922915115748765\n",
            "Answer: phishing, malware, and data breaches\n",
            "\n",
            "Question 2: What is the capital of India?\n",
            "Score: 0.0007901894859969616\n",
            "Answer is not relevant to the question asked.\n",
            "\n",
            "Question 3: What is the Unit of Internet?\n",
            "Score: 0.20774200558662415\n",
            "Answer is not relevant to the question asked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATION FOR THE ABOVE CODE\n",
        "\n",
        "The program evaluates how confidently the QA model answers different types of questions based on a given context.\n",
        "\n",
        "1. For the explicit question (Q1), the context directly contains the answer.\n",
        "\n",
        "  * The model gives a high score (0.92).\n",
        "\n",
        "  * The code prints the actual answer because it passes the 0.8 confidence threshold.\n",
        "\n",
        "2. For the implicit or unrelated questions (Q2 and Q3), the context does not contain the answers.\n",
        "\n",
        "  * The model produces very low scores (0.00 and 0.20).\n",
        "\n",
        "  * Since the scores are below 0.8, the code correctly outputs:\n",
        "  “Answer is not relevant to the question asked.”\n",
        "\n",
        "CONCLUSION\n",
        "\n",
        "The experiment shows that:\n",
        "\n",
        "  * The QA model works accurately only when the question matches the context.\n",
        "\n",
        "  * For questions that are not related to the context, the model gives low confidence scores, and the program successfully identifies them as irrelevant.\n"
      ],
      "metadata": {
        "id": "gbgIhPq8HpyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WEB SCRAPING**"
      ],
      "metadata": {
        "id": "A-csxe0oHtfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright nest_asyncio\n",
        "!playwright install chromium\n",
        "!apt-get install libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 libxcomposite1"
      ],
      "metadata": {
        "id": "fEFD8BluHwIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a10712-4d5d-4698-98ea-3547d596e7be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.12/dist-packages (1.57.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.12/dist-packages (from playwright) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# COMMON IMPORTS & EVENT LOOP FIX\n",
        "# ===============================\n",
        "\n",
        "import asyncio, json, csv, time\n",
        "# asyncio → handles asynchronous execution\n",
        "# json → save data in JSON format\n",
        "# csv → save data in CSV format\n",
        "# time → (optional) used for delays if needed\n",
        "\n",
        "from pathlib import Path\n",
        "# Path → cleaner and OS-independent file path handling\n",
        "\n",
        "import nest_asyncio\n",
        "# nest_asyncio → allows running asyncio inside environments like Google Colab\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# Fixes \"event loop already running\" error in Colab/Jupyter\n",
        "\n",
        "from playwright.async_api import async_playwright\n",
        "# Playwright async API → used for browser automation & scraping JS/AJAX websites\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# TARGET URL\n",
        "# ===============================\n",
        "\n",
        "# AJAX_URL = \"https://webscraper.io/test-sites/e-commerce/ajax\"\n",
        "# AJAX_URL = \"https://webscraper.io/test-sites\"\n",
        "\n",
        "AJAX_URL = \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=1\"\n",
        "# Target page containing laptop products\n",
        "# This page loads content dynamically, so Playwright is required\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ASYNC SCRAPING FUNCTION\n",
        "# ===============================\n",
        "\n",
        "async def scrape_ajax_site():\n",
        "    # Create Playwright context\n",
        "    async with async_playwright() as p:\n",
        "\n",
        "        # Launch Chromium browser in headless mode\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        # headless=True → browser runs in background (no UI)\n",
        "\n",
        "        # Create a fresh browser context\n",
        "        ctx = await browser.new_context()\n",
        "\n",
        "        # Open a new browser page\n",
        "        page = await ctx.new_page()\n",
        "\n",
        "        # Navigate to target URL\n",
        "        await page.goto(AJAX_URL, timeout=60000)\n",
        "        # timeout=60000 → wait up to 60 seconds if page loads slowly\n",
        "\n",
        "        # Wait until product cards are visible\n",
        "        await page.wait_for_selector(\".thumbnail\", timeout=30000)\n",
        "        # Ensures products are fully loaded before scraping\n",
        "\n",
        "\n",
        "        # ===============================\n",
        "        # HANDLE \"LOAD MORE\" BUTTON\n",
        "        # ===============================\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Try to locate the \"Load more\" button\n",
        "                load_more = await page.query_selector(\"button:has-text('Load more')\")\n",
        "\n",
        "                # If button not found → stop loop\n",
        "                if not load_more:\n",
        "                    break\n",
        "\n",
        "                # Check if button is disabled\n",
        "                is_disabled = await load_more.get_attribute(\"disabled\")\n",
        "                if is_disabled:\n",
        "                    break\n",
        "\n",
        "                # Count products before clicking\n",
        "                before_count = len(await page.query_selector_all(\".thumbnail\"))\n",
        "\n",
        "                # Click \"Load more\"\n",
        "                await load_more.click()\n",
        "\n",
        "                # Wait until network requests finish\n",
        "                await page.wait_for_load_state(\"networkidle\")\n",
        "\n",
        "                # Soft wait loop to detect newly loaded products\n",
        "                for _ in range(30):\n",
        "                    after_count = len(await page.query_selector_all(\".thumbnail\"))\n",
        "                    if after_count > before_count:\n",
        "                        break\n",
        "                    await asyncio.sleep(0.2)\n",
        "\n",
        "                # If no new products appeared → exit loop\n",
        "                if after_count <= before_count:\n",
        "                    break\n",
        "\n",
        "            except Exception:\n",
        "                # Any error (button missing, timeout, etc.) → exit loop\n",
        "                break\n",
        "\n",
        "\n",
        "        # ===============================\n",
        "        # EXTRACT PRODUCT DATA\n",
        "        # ===============================\n",
        "\n",
        "        # Get all product cards\n",
        "        cards = await page.query_selector_all(\".thumbnail\")\n",
        "\n",
        "        rows = []  # list to store scraped product data\n",
        "\n",
        "        for card in cards:\n",
        "\n",
        "            # Extract product title and link\n",
        "            title_el = await card.query_selector(\".title\")\n",
        "            title = (await title_el.text_content()).strip() if title_el else None\n",
        "            url = await title_el.get_attribute(\"href\") if title_el else None\n",
        "\n",
        "            # Extract price\n",
        "            price_el = await card.query_selector(\".price\")\n",
        "            price = (await price_el.text_content()).strip() if price_el else None\n",
        "\n",
        "            # Extract rating (count of star icons)\n",
        "            stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
        "            rating = len(stars) if stars else 0\n",
        "\n",
        "            # Extract product image URL\n",
        "            img_el = await card.query_selector(\"img\")\n",
        "            img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
        "\n",
        "            # Store extracted data in dictionary\n",
        "            rows.append({\n",
        "                \"title\": title,\n",
        "                \"price\": price,\n",
        "                \"rating_stars\": rating,\n",
        "                \"product_url\": url,\n",
        "                \"image_url\": img_src\n",
        "            })\n",
        "\n",
        "        # Close browser after scraping\n",
        "        await browser.close()\n",
        "\n",
        "        # Return scraped data\n",
        "        return rows\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# RUN ASYNC FUNCTION\n",
        "# ===============================\n",
        "\n",
        "# Execute async scraping function\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_ajax_site())\n",
        "\n",
        "# Print number of products scraped\n",
        "print(f\"Collected {len(data)} products\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# SAVE DATA TO FILES\n",
        "# ===============================\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "\n",
        "csv_path = Path(\"output/products_ajax.csv\")\n",
        "json_path = Path(\"output/products_ajax.json\")\n",
        "\n",
        "# Save data to CSV\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "# Save data to JSON\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Print confirmation messages\n",
        "print(f\"Saved CSV → {csv_path}\")\n",
        "print(f\"Saved JSON → {json_path}\")"
      ],
      "metadata": {
        "id": "Pk1MIsArHxZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f87ca2f-f5a8-49b4-e518-f15a9a367dbf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 6 products\n",
            "Saved CSV → output/products_ajax.csv\n",
            "Saved JSON → output/products_ajax.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION FOR THE ABOVE CODE**\n",
        "* The program successfully uses **asynchronous web scraping** with **Playwright** to extract product data from a **dynamic (AJAX-based) e-commerce webpage**.\n",
        "* The **event loop issue** commonly seen in Jupyter/Google Colab environments is handled correctly using `nest_asyncio`, allowing the async Playwright code to execute without errors.\n",
        "* The script navigates to the target laptop listing page and **waits explicitly for product cards (`.thumbnail`) to load**, ensuring data is scraped only after the page content becomes visible.\n",
        "* The **“Load more” button logic** is implemented safely using a loop that:\n",
        "\n",
        "  * Checks if the button exists,\n",
        "  * Verifies whether it is disabled,\n",
        "  * Confirms that new products are actually loaded after each click.\n",
        "* On the given page, **no additional products were dynamically loaded**, so the loop exited gracefully without errors.\n",
        "\n",
        "---\n",
        "\n",
        "**Output Analysis**\n",
        "\n",
        "* **Total products collected:** `6`\n",
        "\n",
        "* Each product record includes:\n",
        "\n",
        "  * `title` – name of the laptop\n",
        "  * `price` – listed price\n",
        "  * `rating_stars` – number of star icons found\n",
        "  * `product_url` – link to the product page\n",
        "  * `image_url` – URL of the product image\n",
        "\n",
        "* The scraped data was **successfully persisted in two formats**:\n",
        "\n",
        "  * **CSV file:** `output/products_ajax.csv`\n",
        "  * **JSON file:** `output/products_ajax.json`\n",
        "\n",
        "* Saving the data in **both structured (CSV)** and **semi-structured (JSON)** formats makes it suitable for:\n",
        "\n",
        "  * Data analysis (Excel, Pandas)\n",
        "  * APIs or dashboards\n",
        "  * Machine learning or pricing analytics pipelines\n",
        "\n"
      ],
      "metadata": {
        "id": "XSJOrU-XH2Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# COMMON IMPORTS & COLAB EVENT LOOP FIX\n",
        "# ===============================\n",
        "\n",
        "import asyncio, json, csv, time\n",
        "# asyncio → to run asynchronous Playwright code\n",
        "# json → to save scraped data as JSON\n",
        "# csv → to save scraped data as CSV\n",
        "# time → utility module (not heavily used here)\n",
        "\n",
        "from pathlib import Path\n",
        "# Path → platform-independent file and folder handling\n",
        "\n",
        "import nest_asyncio\n",
        "# nest_asyncio → required to run asyncio inside Jupyter/Colab\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# Fixes \"event loop is already running\" error in Colab\n",
        "\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "# async_playwright → Playwright async browser controller\n",
        "# PlaywrightTimeoutError → used to safely stop pagination when no page loads\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# BASE URL TEMPLATE\n",
        "# ===============================\n",
        "\n",
        "# Base URL with page number placeholder\n",
        "BASE_URL = \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page={page}\"\n",
        "# We will dynamically replace {page} with page numbers (1, 2, 3, ...)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ASYNC SCRAPING FUNCTION\n",
        "# ===============================\n",
        "\n",
        "async def scrape_all_pages_auto():\n",
        "    # Start Playwright\n",
        "    async with async_playwright() as p:\n",
        "\n",
        "        # Launch Chromium browser in headless mode\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        # headless=True → browser runs without GUI (faster, interview-friendly)\n",
        "\n",
        "        # Create a fresh browser context\n",
        "        ctx = await browser.new_context()\n",
        "\n",
        "        # Open a new page/tab\n",
        "        page = await ctx.new_page()\n",
        "\n",
        "        all_rows = []\n",
        "        # all_rows → stores products collected from all pages\n",
        "\n",
        "        page_num = 1\n",
        "        # Start scraping from page 1\n",
        "\n",
        "\n",
        "        # ===============================\n",
        "        # PAGINATION LOOP\n",
        "        # ===============================\n",
        "\n",
        "        while True:\n",
        "            # Create page-specific URL\n",
        "            url = BASE_URL.format(page=page_num)\n",
        "\n",
        "            print(f\"Visiting page {page_num}: {url}\")\n",
        "\n",
        "            # Navigate to the page\n",
        "            await page.goto(url, timeout=60000)\n",
        "            # timeout=60000 → wait up to 60 seconds for page load\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # WAIT FOR PRODUCTS\n",
        "            # ===============================\n",
        "\n",
        "            try:\n",
        "                # Wait until product cards appear\n",
        "                await page.wait_for_selector(\".thumbnail\", timeout=15000)\n",
        "            except PlaywrightTimeoutError:\n",
        "                # If no products load within time → stop pagination\n",
        "                print(f\"No products found (timeout) on page {page_num}, stopping.\")\n",
        "                break\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # EXTRACT PRODUCT CARDS\n",
        "            # ===============================\n",
        "\n",
        "            cards = await page.query_selector_all(\".thumbnail\")\n",
        "            # Select all product blocks on the page\n",
        "\n",
        "            if not cards:\n",
        "                # Safety check: if no products found → stop\n",
        "                print(f\"No .thumbnail elements on page {page_num}, stopping.\")\n",
        "                break\n",
        "\n",
        "            page_rows = []\n",
        "            # Stores products from the current page only\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # LOOP THROUGH PRODUCTS\n",
        "            # ===============================\n",
        "\n",
        "            for card in cards:\n",
        "\n",
        "                # Extract product title and link\n",
        "                title_el = await card.query_selector(\".title\")\n",
        "                title = (await title_el.text_content()).strip() if title_el else None\n",
        "                url = await title_el.get_attribute(\"href\") if title_el else None\n",
        "\n",
        "                # Extract price\n",
        "                price_el = await card.query_selector(\".price\")\n",
        "                price = (await price_el.text_content()).strip() if price_el else None\n",
        "\n",
        "                # Extract rating by counting star icons\n",
        "                stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
        "                rating = len(stars) if stars else 0\n",
        "\n",
        "                # Extract product image URL\n",
        "                img_el = await card.query_selector(\"img\")\n",
        "                img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
        "\n",
        "                # Store extracted data in dictionary\n",
        "                page_rows.append({\n",
        "                    \"title\": title,\n",
        "                    \"price\": price,\n",
        "                    \"rating_stars\": rating,\n",
        "                    \"product_url\": url,\n",
        "                    \"image_url\": img_src,\n",
        "                    \"page\": page_num  # helps identify source page\n",
        "                })\n",
        "\n",
        "\n",
        "            # Log how many products were scraped from this page\n",
        "            print(f\"Page {page_num}: collected {len(page_rows)} products\")\n",
        "\n",
        "            # Add current page data to global list\n",
        "            all_rows.extend(page_rows)\n",
        "\n",
        "            # Move to next page\n",
        "            page_num += 1\n",
        "\n",
        "\n",
        "        # Close browser after scraping all pages\n",
        "        await browser.close()\n",
        "\n",
        "        # Return all collected products\n",
        "        return all_rows\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# RUN ASYNC FUNCTION\n",
        "# ===============================\n",
        "\n",
        "# Execute the async scraping function\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_all_pages_auto())\n",
        "\n",
        "# Print total number of products scraped\n",
        "print(f\"Total collected products from all pages: {len(data)}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# SAVE OUTPUT FILES\n",
        "# ===============================\n",
        "\n",
        "# Create output directory if it does not exist\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "\n",
        "csv_path = Path(\"output/products_all_pages.csv\")\n",
        "json_path = Path(\"output/products_all_pages.json\")\n",
        "\n",
        "if data:\n",
        "    # Save data to CSV\n",
        "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    # Save data to JSON\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved CSV → {csv_path}\")\n",
        "    print(f\"Saved JSON → {json_path}\")\n",
        "else:\n",
        "    # If nothing was scraped\n",
        "    print(\"No data scraped, nothing saved.\")"
      ],
      "metadata": {
        "id": "7SzNc3-vH4tD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28c4567-5fd7-4371-b3e8-383bc4b2dcbf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visiting page 1: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=1\n",
            "Page 1: collected 6 products\n",
            "Visiting page 2: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=2\n",
            "Page 2: collected 6 products\n",
            "Visiting page 3: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=3\n",
            "Page 3: collected 6 products\n",
            "Visiting page 4: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=4\n",
            "Page 4: collected 6 products\n",
            "Visiting page 5: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=5\n",
            "Page 5: collected 6 products\n",
            "Visiting page 6: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=6\n",
            "Page 6: collected 6 products\n",
            "Visiting page 7: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=7\n",
            "Page 7: collected 6 products\n",
            "Visiting page 8: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=8\n",
            "Page 8: collected 6 products\n",
            "Visiting page 9: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=9\n",
            "Page 9: collected 6 products\n",
            "Visiting page 10: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=10\n",
            "Page 10: collected 6 products\n",
            "Visiting page 11: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=11\n",
            "Page 11: collected 6 products\n",
            "Visiting page 12: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=12\n",
            "Page 12: collected 6 products\n",
            "Visiting page 13: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=13\n",
            "Page 13: collected 6 products\n",
            "Visiting page 14: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=14\n",
            "Page 14: collected 6 products\n",
            "Visiting page 15: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=15\n",
            "Page 15: collected 6 products\n",
            "Visiting page 16: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=16\n",
            "Page 16: collected 6 products\n",
            "Visiting page 17: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=17\n",
            "Page 17: collected 6 products\n",
            "Visiting page 18: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=18\n",
            "Page 18: collected 6 products\n",
            "Visiting page 19: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=19\n",
            "Page 19: collected 6 products\n",
            "Visiting page 20: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=20\n",
            "Page 20: collected 3 products\n",
            "Visiting page 21: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=21\n",
            "No products found (timeout) on page 21, stopping.\n",
            "Total collected products from all pages: 117\n",
            "Saved CSV → output/products_all_pages.csv\n",
            "Saved JSON → output/products_all_pages.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION FOR THE ABOVE CODE**\n",
        "\n",
        "* The program successfully implements **automated multi-page web scraping** using **asynchronous Playwright** on a **paginated e-commerce website**.\n",
        "* The **event-loop conflict in Jupyter/Google Colab** is correctly handled using `nest_asyncio`, allowing async browser automation to run smoothly without runtime errors.\n",
        "* The scraper dynamically constructs URLs using a **page number template**, enabling it to visit pages sequentially (`page=1` to `page=20`) without hard-coding page links.\n",
        "* For each page:\n",
        "\n",
        "  * The script waits explicitly for `.thumbnail` product cards to load, ensuring reliable extraction even on slow networks.\n",
        "  * It extracts **structured product information**, including:\n",
        "\n",
        "    * Product title\n",
        "    * Price\n",
        "    * Rating (computed by counting star icons)\n",
        "    * Product URL\n",
        "    * Image URL\n",
        "    * Page number (source traceability)\n",
        "* The **pagination stopping condition** is robust:\n",
        "\n",
        "  * When page 21 fails to load products within the timeout period, a `PlaywrightTimeoutError` is triggered.\n",
        "  * This is safely caught and used as a **natural termination signal**, preventing infinite looping.\n",
        "* The scraper maintains **data integrity** by:\n",
        "\n",
        "  * Collecting products page-wise\n",
        "  * Merging all records into a single dataset after pagination ends\n",
        "\n",
        "---\n",
        "\n",
        "**Output Analysis**\n",
        "\n",
        "* **Pages successfully scraped:** `1 – 20`\n",
        "\n",
        "* **Products per page:**\n",
        "\n",
        "  * Pages 1–19 → 6 products each\n",
        "  * Page 20 → 3 products\n",
        "\n",
        "* **Total products collected:**\n",
        "  **117 laptop products**\n",
        "\n",
        "* The final dataset was saved in two commonly used formats:\n",
        "\n",
        "  * **CSV:** `output/products_all_pages.csv` (ideal for Excel, Pandas analysis)\n",
        "  * **JSON:** `output/products_all_pages.json` (ideal for APIs, web apps, ML pipelines)\n",
        "\n",
        "* The consistent logging confirms:\n",
        "\n",
        "  * Correct pagination handling\n",
        "  * No data duplication\n",
        "  * Graceful termination after the last valid page\n"
      ],
      "metadata": {
        "id": "IbwCPxWNH8MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# COMMON IMPORTS & COLAB EVENT LOOP FIX\n",
        "# ===============================\n",
        "\n",
        "import asyncio, json, csv, time\n",
        "# asyncio → run asynchronous Playwright code\n",
        "# json → save scraped data in JSON format\n",
        "# csv → save scraped data in CSV format\n",
        "# time → utility module (optional, for delays if needed)\n",
        "\n",
        "from pathlib import Path\n",
        "# Path → OS-independent way to handle files and folders\n",
        "\n",
        "import nest_asyncio\n",
        "# nest_asyncio → required when running asyncio in Jupyter / Google Colab\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# Fixes \"event loop is already running\" error in Colab\n",
        "\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "# async_playwright → async browser automation library\n",
        "# PlaywrightTimeoutError → used to stop scraping when pages no longer load\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# BASE URL FOR TABLETS (PAGINATED)\n",
        "# ===============================\n",
        "\n",
        "# Base URL for tablet products\n",
        "# {page} will be replaced with page numbers (1, 2, 3, ...)\n",
        "BASE_URL = \"https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page={page}\"\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ASYNC SCRAPING FUNCTION\n",
        "# ===============================\n",
        "\n",
        "async def scrape_tablets_all_pages():\n",
        "    # Start Playwright context\n",
        "    async with async_playwright() as p:\n",
        "\n",
        "        # Launch Chromium browser in headless mode\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        # headless=True → browser runs without UI (faster & automation-friendly)\n",
        "\n",
        "        # Create a new browser context (isolated session)\n",
        "        ctx = await browser.new_context()\n",
        "\n",
        "        # Open a new browser page\n",
        "        page = await ctx.new_page()\n",
        "\n",
        "        all_rows = []\n",
        "        # all_rows → stores products scraped from ALL pages\n",
        "\n",
        "        page_num = 1\n",
        "        # Start scraping from page 1\n",
        "\n",
        "\n",
        "        # ===============================\n",
        "        # PAGINATION LOOP\n",
        "        # ===============================\n",
        "\n",
        "        while True:\n",
        "            # Generate URL for current page number\n",
        "            url = BASE_URL.format(page=page_num)\n",
        "\n",
        "            print(f\"Visiting page {page_num}: {url}\")\n",
        "\n",
        "            # Navigate to the page\n",
        "            await page.goto(url, timeout=60000)\n",
        "            # timeout=60000 → wait up to 60 seconds for page load\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # WAIT FOR PRODUCTS TO LOAD\n",
        "            # ===============================\n",
        "\n",
        "            try:\n",
        "                # Wait until product cards appear on the page\n",
        "                await page.wait_for_selector(\".thumbnail\", timeout=15000)\n",
        "            except PlaywrightTimeoutError:\n",
        "                # If products do not load → assume no more pages\n",
        "                print(f\"No products found on page {page_num} (timeout), stopping.\")\n",
        "                break\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # EXTRACT PRODUCT CARDS\n",
        "            # ===============================\n",
        "\n",
        "            # Select all product blocks\n",
        "            cards = await page.query_selector_all(\".thumbnail\")\n",
        "\n",
        "            if not cards:\n",
        "                # Safety check: stop if no products are found\n",
        "                print(f\"No .thumbnail elements on page {page_num}, stopping.\")\n",
        "                break\n",
        "\n",
        "            page_rows = []\n",
        "            # page_rows → stores products from the CURRENT page only\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # LOOP THROUGH EACH PRODUCT\n",
        "            # ===============================\n",
        "\n",
        "            for card in cards:\n",
        "\n",
        "                # Extract product title and product link\n",
        "                title_el = await card.query_selector(\".title\")\n",
        "                title = (await title_el.text_content()).strip() if title_el else None\n",
        "                url = await title_el.get_attribute(\"href\") if title_el else None\n",
        "\n",
        "                # Extract product price\n",
        "                price_el = await card.query_selector(\".price\")\n",
        "                price = (await price_el.text_content()).strip() if price_el else None\n",
        "\n",
        "                # Extract rating by counting star icons\n",
        "                stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
        "                rating = len(stars) if stars else 0\n",
        "\n",
        "                # Extract product image URL\n",
        "                img_el = await card.query_selector(\"img\")\n",
        "                img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
        "\n",
        "                # Store extracted data in dictionary format\n",
        "                page_rows.append({\n",
        "                    \"title\": title,\n",
        "                    \"price\": price,\n",
        "                    \"rating_stars\": rating,\n",
        "                    \"product_url\": url,\n",
        "                    \"image_url\": img_src,\n",
        "                    \"page\": page_num  # helps track source page\n",
        "                })\n",
        "\n",
        "\n",
        "            # Log number of products scraped from this page\n",
        "            print(f\"Page {page_num}: collected {len(page_rows)} products\")\n",
        "\n",
        "            # Add current page data to the global list\n",
        "            all_rows.extend(page_rows)\n",
        "\n",
        "            # Move to the next page\n",
        "            page_num += 1\n",
        "\n",
        "\n",
        "        # Close the browser after scraping all pages\n",
        "        await browser.close()\n",
        "\n",
        "        # Return all scraped tablet products\n",
        "        return all_rows\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# RUN ASYNC FUNCTION\n",
        "# ===============================\n",
        "\n",
        "# Execute the async scraping function\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_tablets_all_pages())\n",
        "\n",
        "# Print total number of products scraped\n",
        "print(f\"Total collected products from all tablet pages: {len(data)}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# SAVE OUTPUT FILES\n",
        "# ===============================\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "\n",
        "csv_path = Path(\"output/tablets_all_pages.csv\")\n",
        "json_path = Path(\"output/tablets_all_pages.json\")\n",
        "\n",
        "if data:\n",
        "    # Save scraped data to CSV file\n",
        "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    # Save scraped data to JSON file\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved CSV → {csv_path}\")\n",
        "    print(f\"Saved JSON → {json_path}\")\n",
        "else:\n",
        "    # If no data was scraped\n",
        "    print(\"No data scraped, nothing saved.\")"
      ],
      "metadata": {
        "id": "3XqxfLLiH_Ks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ebbf0b-483b-4e3a-88f7-18a5c6a38190"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visiting page 1: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=1\n",
            "Page 1: collected 6 products\n",
            "Visiting page 2: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=2\n",
            "Page 2: collected 6 products\n",
            "Visiting page 3: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=3\n",
            "Page 3: collected 6 products\n",
            "Visiting page 4: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=4\n",
            "Page 4: collected 3 products\n",
            "Visiting page 5: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=5\n",
            "No products found on page 5 (timeout), stopping.\n",
            "Total collected products from all tablet pages: 21\n",
            "Saved CSV → output/tablets_all_pages.csv\n",
            "Saved JSON → output/tablets_all_pages.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION FOR THE ABOVE CODE**\n",
        "* The program successfully performs **automated multi-page web scraping** of **tablet products** using **asynchronous Playwright**.\n",
        "* The use of `nest_asyncio` effectively resolves the **event loop conflict** encountered in Jupyter Notebook or Google Colab environments, allowing async browser automation to execute smoothly.\n",
        "* A **paginated URL template** is used to dynamically generate page URLs, enabling the scraper to automatically navigate through multiple pages without manual intervention.\n",
        "* For each page:\n",
        "\n",
        "  * The script explicitly waits for the `.thumbnail` selector, ensuring that product cards are fully loaded before extraction.\n",
        "  * It extracts structured product information including:\n",
        "\n",
        "    * Product title\n",
        "    * Price\n",
        "    * Rating (calculated by counting star icons)\n",
        "    * Product URL\n",
        "    * Image URL\n",
        "    * Page number (for source traceability)\n",
        "* The scraper uses a **timeout-based stopping condition**:\n",
        "\n",
        "  * When page 5 fails to load product cards within the specified timeout, a `PlaywrightTimeoutError` is raised.\n",
        "  * This exception is safely handled and used to **terminate pagination gracefully**, preventing unnecessary requests or infinite looping.\n",
        "* Data is collected page-wise and merged into a single dataset, ensuring **no duplication and complete coverage** of available tablet listings.\n",
        "\n",
        "---\n",
        "\n",
        " **Output Analysis**\n",
        "\n",
        "* **Pages successfully scraped:** `1 – 4`\n",
        "\n",
        "* **Products per page:**\n",
        "\n",
        "  * Pages 1–3 → 6 products each\n",
        "  * Page 4 → 3 products\n",
        "\n",
        "* **Total tablet products collected:**\n",
        "  **21 products**\n",
        "\n",
        "* The extracted data was successfully saved in two widely used formats:\n",
        "\n",
        "  * **CSV:** `output/tablets_all_pages.csv` – suitable for spreadsheets and data analysis\n",
        "  * **JSON:** `output/tablets_all_pages.json` – suitable for APIs, dashboards, and machine learning workflows\n",
        "\n",
        "* Console logs confirm:\n",
        "\n",
        "  * Correct pagination traversal\n",
        "  * Accurate product counts per page\n",
        "  * Proper termination after the last available page"
      ],
      "metadata": {
        "id": "5eAM4gbTIEaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# SIMPLE PHONES SCRAPER (FINAL)\n",
        "# -----------------------------\n",
        "# This script scrapes phone products from a single-page e-commerce site\n",
        "# using Playwright (async browser automation)\n",
        "\n",
        "import asyncio, json, csv\n",
        "# asyncio → to run asynchronous Playwright code\n",
        "# json → to save scraped data in JSON format\n",
        "# csv → to save scraped data in CSV format\n",
        "\n",
        "from pathlib import Path\n",
        "# Path → OS-independent file and directory handling\n",
        "\n",
        "import nest_asyncio\n",
        "# nest_asyncio → allows asyncio to run inside Jupyter / Google Colab\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# Fixes \"event loop already running\" error in Colab\n",
        "\n",
        "from urllib.parse import urljoin\n",
        "# urljoin → safely combine base URL with relative URLs\n",
        "\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "# async_playwright → async Playwright API\n",
        "# PlaywrightTimeoutError → handles timeout when elements do not load\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# WEBSITE CONFIGURATION\n",
        "# -----------------------------\n",
        "\n",
        "# Phones category has ONLY ONE PAGE → no pagination required\n",
        "BASE_URL = \"https://webscraper.io/test-sites/e-commerce/static/phones\"\n",
        "\n",
        "# Root URL used to convert relative URLs into absolute URLs\n",
        "ROOT = \"https://webscraper.io\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ASYNC SCRAPING FUNCTION\n",
        "# -----------------------------\n",
        "\n",
        "async def scrape_phones():\n",
        "    # Start Playwright context\n",
        "    async with async_playwright() as p:\n",
        "\n",
        "        # Launch Chromium browser in headless mode\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        # headless=True → browser runs without UI (faster, automation-friendly)\n",
        "\n",
        "        # Create a new isolated browser session\n",
        "        ctx = await browser.new_context()\n",
        "\n",
        "        # Open a new browser page\n",
        "        page = await ctx.new_page()\n",
        "\n",
        "        print(\"Visiting:\", BASE_URL)\n",
        "\n",
        "        # Navigate to the phones page\n",
        "        await page.goto(BASE_URL, timeout=60000)\n",
        "        # timeout=60000 → wait up to 60 seconds for page load\n",
        "\n",
        "\n",
        "        # -----------------------------\n",
        "        # WAIT FOR PRODUCTS TO LOAD\n",
        "        # -----------------------------\n",
        "\n",
        "        try:\n",
        "            # Wait until product cards appear\n",
        "            await page.wait_for_selector(\".thumbnail\", timeout=6000)\n",
        "        except PlaywrightTimeoutError:\n",
        "            # If products do not load, return empty list\n",
        "            print(\"No products found.\")\n",
        "            return []\n",
        "\n",
        "\n",
        "        # Select all product cards\n",
        "        cards = await page.query_selector_all(\".thumbnail\")\n",
        "\n",
        "        all_rows = []\n",
        "        # all_rows → list to store scraped product data\n",
        "\n",
        "\n",
        "        # -----------------------------\n",
        "        # EXTRACT DATA FROM EACH PRODUCT\n",
        "        # -----------------------------\n",
        "\n",
        "        for card in cards:\n",
        "\n",
        "            # Extract product title and relative URL\n",
        "            title_el = await card.query_selector(\".title\")\n",
        "            title = (await title_el.text_content()).strip()\n",
        "            href = await title_el.get_attribute(\"href\")\n",
        "\n",
        "            # Convert relative product URL to absolute URL\n",
        "            full_product_url = urljoin(ROOT, href)\n",
        "\n",
        "            # Extract product price\n",
        "            price_el = await card.query_selector(\".price\")\n",
        "            price = (await price_el.text_content()).strip()\n",
        "\n",
        "            # Extract rating by counting star icons\n",
        "            stars = await card.query_selector_all(\".glyphicon-star\")\n",
        "            rating = len(stars)\n",
        "\n",
        "            # Extract image URL and convert to absolute URL\n",
        "            img_el = await card.query_selector(\"img\")\n",
        "            img_url = urljoin(ROOT, await img_el.get_attribute(\"src\"))\n",
        "\n",
        "            # Store extracted data in dictionary\n",
        "            all_rows.append({\n",
        "                \"title\": title,\n",
        "                \"price\": price,\n",
        "                \"rating_stars\": rating,\n",
        "                \"product_url\": full_product_url,\n",
        "                \"image_url\": img_url\n",
        "            })\n",
        "\n",
        "\n",
        "        # Close the browser after scraping\n",
        "        await browser.close()\n",
        "\n",
        "        # Return all scraped phone products\n",
        "        return all_rows\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# RUN THE SCRAPER\n",
        "# -------------------------------------------------\n",
        "\n",
        "# Execute the async scraping function\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_phones())\n",
        "\n",
        "# Print total number of products scraped\n",
        "print(f\"Total products scraped: {len(data)}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# SAVE DATA TO CSV + JSON\n",
        "# -------------------------------------------------\n",
        "\n",
        "# Create output directory if it does not exist\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "# Save data as CSV\n",
        "csv_path = Path(\"output/phones_simple.csv\")\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(\"Saved CSV:\", csv_path)\n",
        "\n",
        "\n",
        "# Save data as JSON\n",
        "json_path = Path(\"output/phones_simple.json\")\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved JSON:\", json_path)"
      ],
      "metadata": {
        "id": "eIBzp7BWIHOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa27c60-d35c-4865-9b0c-5c527b27931a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visiting: https://webscraper.io/test-sites/e-commerce/static/phones\n",
            "Total products scraped: 3\n",
            "Saved CSV: output/phones_simple.csv\n",
            "Saved JSON: output/phones_simple.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION FOR THE ABOVE CODE**\n",
        "* The script successfully implements a **simple asynchronous web scraper** using **Playwright** to extract phone product data from a **single-page e-commerce website**.\n",
        "* Since the phones category contains **only one page**, no pagination logic is required, making the scraper lightweight and efficient.\n",
        "* The use of `nest_asyncio` correctly resolves the **event loop conflict** in Jupyter Notebook / Google Colab environments, allowing asynchronous browser automation to execute without errors.\n",
        "* The program waits explicitly for the `.thumbnail` selector to ensure that all product cards are fully loaded before data extraction.\n",
        "* For each phone product, the scraper accurately extracts:\n",
        "\n",
        "  * Product title\n",
        "  * Price\n",
        "  * Rating (calculated by counting star icons)\n",
        "  * Product URL (converted from relative to absolute using `urljoin`)\n",
        "  * Image URL (converted to absolute URL)\n",
        "* The browser is properly closed after scraping, ensuring **efficient resource management**.\n",
        "\n",
        "---\n",
        "\n",
        "**Output Analysis**\n",
        "\n",
        "* **Total products scraped:** `3 phone products`\n",
        "* All available phone listings on the page were successfully collected without duplication or data loss.\n",
        "* The extracted data was saved in two formats:\n",
        "\n",
        "  * **CSV:** `output/phones_simple.csv` – suitable for spreadsheets and data analysis\n",
        "  * **JSON:** `output/phones_simple.json` – suitable for APIs, dashboards, and further processing\n",
        "* Console output confirms:\n",
        "\n",
        "  * Successful page access\n",
        "  * Correct product count\n",
        "  * Successful file generation"
      ],
      "metadata": {
        "id": "khxWNFSUIKQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# COMMON IMPORTS & COLAB EVENT LOOP FIX\n",
        "# ===============================\n",
        "\n",
        "import asyncio, json, csv, time\n",
        "# asyncio → run asynchronous Playwright code\n",
        "# json → save scraped data in JSON format\n",
        "# csv → save scraped data in CSV format\n",
        "# time → utility module (not directly used here, but commonly included)\n",
        "\n",
        "from pathlib import Path\n",
        "# Path → OS-independent way to handle files and folders\n",
        "\n",
        "import nest_asyncio\n",
        "# nest_asyncio → required to run asyncio inside Jupyter / Google Colab\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# Fixes \"event loop already running\" error in Colab\n",
        "\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "# async_playwright → async browser automation using Playwright\n",
        "# PlaywrightTimeoutError → used to safely stop scraping when pages don’t load\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# BASE URL FOR TOUCH PHONES\n",
        "# ===============================\n",
        "\n",
        "# Base URL for touch phones category\n",
        "# {page} will be dynamically replaced with page numbers (1, 2, 3, ...)\n",
        "BASE_URL = \"https://webscraper.io/test-sites/e-commerce/static/phones/touch?page={page}\"\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ASYNC SCRAPING FUNCTION\n",
        "# ===============================\n",
        "\n",
        "async def scrape_touch_phones_all_pages():\n",
        "    # Start Playwright engine\n",
        "    async with async_playwright() as p:\n",
        "\n",
        "        # Launch Chromium browser in headless mode\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        # headless=True → runs browser in background (no UI)\n",
        "\n",
        "        # Create a fresh browser context (isolated session)\n",
        "        ctx = await browser.new_context()\n",
        "\n",
        "        # Open a new browser page/tab\n",
        "        page = await ctx.new_page()\n",
        "\n",
        "        all_rows = []\n",
        "        # all_rows → stores products from ALL pages\n",
        "\n",
        "        page_num = 1\n",
        "        # Start scraping from page 1\n",
        "\n",
        "\n",
        "        # ===============================\n",
        "        # PAGINATION LOOP\n",
        "        # ===============================\n",
        "\n",
        "        while True:\n",
        "            # Generate URL for the current page\n",
        "            url = BASE_URL.format(page=page_num)\n",
        "\n",
        "            print(f\"Visiting page {page_num}: {url}\")\n",
        "\n",
        "            # Navigate to the page\n",
        "            await page.goto(url, timeout=60000)\n",
        "            # timeout=60000 → wait up to 60 seconds for page to load\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # WAIT FOR PRODUCTS TO LOAD\n",
        "            # ===============================\n",
        "\n",
        "            try:\n",
        "                # Wait until product cards appear on the page\n",
        "                await page.wait_for_selector(\".thumbnail\", timeout=15000)\n",
        "            except PlaywrightTimeoutError:\n",
        "                # If products don’t load, assume no more pages\n",
        "                print(f\"No products found on page {page_num} (timeout), stopping.\")\n",
        "                break\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # EXTRACT PRODUCT CARDS\n",
        "            # ===============================\n",
        "\n",
        "            # Select all product containers on the page\n",
        "            cards = await page.query_selector_all(\".thumbnail\")\n",
        "\n",
        "            if not cards:\n",
        "                # Safety check: stop if no product elements found\n",
        "                print(f\"No .thumbnail elements on page {page_num}, stopping.\")\n",
        "                break\n",
        "\n",
        "            page_rows = []\n",
        "            # page_rows → stores products from the CURRENT page only\n",
        "\n",
        "\n",
        "            # ===============================\n",
        "            # LOOP THROUGH EACH PRODUCT\n",
        "            # ===============================\n",
        "\n",
        "            for card in cards:\n",
        "\n",
        "                # Extract product title and relative URL\n",
        "                title_el = await card.query_selector(\".title\")\n",
        "                title = (await title_el.text_content()).strip() if title_el else None\n",
        "                url = await title_el.get_attribute(\"href\") if title_el else None\n",
        "\n",
        "                # Extract product price\n",
        "                price_el = await card.query_selector(\".price\")\n",
        "                price = (await price_el.text_content()).strip() if price_el else None\n",
        "\n",
        "                # Extract rating by counting star icons\n",
        "                stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
        "                rating = len(stars) if stars else 0\n",
        "\n",
        "                # Extract image URL\n",
        "                img_el = await card.query_selector(\"img\")\n",
        "                img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
        "\n",
        "                # Store extracted data in dictionary\n",
        "                page_rows.append({\n",
        "                    \"title\": title,\n",
        "                    \"price\": price,\n",
        "                    \"rating_stars\": rating,\n",
        "                    \"product_url\": url,\n",
        "                    \"image_url\": img_src,\n",
        "                    \"page\": page_num  # helps track source page\n",
        "                })\n",
        "\n",
        "\n",
        "            # Log how many products were scraped from this page\n",
        "            print(f\"Page {page_num}: collected {len(page_rows)} products\")\n",
        "\n",
        "            # Add current page products to final list\n",
        "            all_rows.extend(page_rows)\n",
        "\n",
        "            # Move to the next page\n",
        "            page_num += 1\n",
        "\n",
        "\n",
        "        # Close browser after scraping all pages\n",
        "        await browser.close()\n",
        "\n",
        "        # Return all scraped touch phone products\n",
        "        return all_rows\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# RUN ASYNC FUNCTION\n",
        "# ===============================\n",
        "\n",
        "# Execute the async scraping function\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_touch_phones_all_pages())\n",
        "\n",
        "# Print total number of products scraped\n",
        "print(f\"Total collected products from all touch phone pages: {len(data)}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# SAVE OUTPUT FILES\n",
        "# ===============================\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "\n",
        "csv_path = Path(\"output/touch_phones_all_pages.csv\")\n",
        "json_path = Path(\"output/touch_phones_all_pages.json\")\n",
        "\n",
        "if data:\n",
        "    # Save data to CSV\n",
        "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    # Save data to JSON\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved CSV → {csv_path}\")\n",
        "    print(f\"Saved JSON → {json_path}\")\n",
        "else:\n",
        "    # If no data was scraped\n",
        "    print(\"No data scraped, nothing saved.\")"
      ],
      "metadata": {
        "id": "tvKOv_8pINIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63092b2b-265e-4917-affa-74177d91929d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visiting page 1: https://webscraper.io/test-sites/e-commerce/static/phones/touch?page=1\n",
            "Page 1: collected 6 products\n",
            "Visiting page 2: https://webscraper.io/test-sites/e-commerce/static/phones/touch?page=2\n",
            "Page 2: collected 3 products\n",
            "Visiting page 3: https://webscraper.io/test-sites/e-commerce/static/phones/touch?page=3\n",
            "No products found on page 3 (timeout), stopping.\n",
            "Total collected products from all touch phone pages: 9\n",
            "Saved CSV → output/touch_phones_all_pages.csv\n",
            "Saved JSON → output/touch_phones_all_pages.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION FOR THE ABOVE CODE**\n",
        "* The program successfully performs **automated multi-page web scraping** of **touch phone products** using **asynchronous Playwright browser automation**.\n",
        "* The use of `nest_asyncio` effectively resolves the **event-loop conflict** in Jupyter Notebook / Google Colab environments, allowing asynchronous code to execute without runtime errors.\n",
        "* A **paginated URL template** is used to dynamically generate page URLs, enabling the scraper to automatically navigate through multiple pages of touch phone listings.\n",
        "* For each page:\n",
        "\n",
        "  * The script explicitly waits for the `.thumbnail` selector, ensuring that product cards are fully loaded before data extraction.\n",
        "  * It extracts structured product information including:\n",
        "\n",
        "    * Product title\n",
        "    * Price\n",
        "    * Rating (computed by counting star icons)\n",
        "    * Product URL\n",
        "    * Image URL\n",
        "    * Page number (to track the source page)\n",
        "* The scraper employs a **timeout-based stopping condition**:\n",
        "\n",
        "  * When page 3 fails to load product cards within the specified timeout, a `PlaywrightTimeoutError` is raised.\n",
        "  * This exception is safely handled to **terminate pagination gracefully**, preventing unnecessary requests or infinite loops.\n",
        "* All scraped data is accumulated into a single dataset, ensuring **complete coverage and no duplication** of available touch phone listings.\n",
        "\n",
        "---\n",
        "\n",
        "**Output Analysis**\n",
        "\n",
        "* **Pages successfully scraped:** `1 – 2`\n",
        "\n",
        "* **Products per page:**\n",
        "\n",
        "  * Page 1 → 6 products\n",
        "  * Page 2 → 3 products\n",
        "\n",
        "* **Total touch phone products collected:**\n",
        "  **9 products**\n",
        "\n",
        "* The extracted data was successfully stored in two formats:\n",
        "\n",
        "  * **CSV:** `output/touch_phones_all_pages.csv` – suitable for spreadsheets and data analysis\n",
        "  * **JSON:** `output/touch_phones_all_pages.json` – suitable for APIs, dashboards, and further processing\n",
        "\n",
        "* Console logs confirm:\n",
        "\n",
        "  * Correct pagination handling\n",
        "  * Accurate product counts per page\n",
        "  * Proper termination after the final available page"
      ],
      "metadata": {
        "id": "bIngVoiUIQY_"
      }
    }
  ]
}